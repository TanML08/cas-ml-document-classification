{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764f43b8",
   "metadata": {},
   "source": [
    "# 03b – DistilBERT Fine-Tuning (BBC News) – Split 80/10/10\n",
    "\n",
    "Ziel: Fine-Tuning von DistilBERT für Textklassifikation und Vergleich gegen Baseline und BERT.\n",
    "\n",
    "**Split-Strategie:** 80% Train / 10% Validation / 10% Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6eb0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8a66f",
   "metadata": {},
   "source": [
    "## Seeds & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae594a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SPLIT_NAME = \"80-10-10\"\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Split: {SPLIT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80a9c4",
   "metadata": {},
   "source": [
    "## 1) CSV laden & Labels encoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee70da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CSV_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"bbc_news.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
    "df.shape, df[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac05302",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sorted = sorted(df[\"label\"].unique())\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels_sorted)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "labels_sorted, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad68b9",
   "metadata": {},
   "source": [
    "## 2) Train/Validation/Test Split (80/10/10)\n",
    "\n",
    "**Identisch zum BERT-Notebook für fairen Vergleich!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"].tolist()\n",
    "y = df[\"label_id\"].tolist()\n",
    "\n",
    "# Erst Test-Set abspalten (10%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Dann Val-Set aus dem Rest (~11.1% von 90% = 10% vom Gesamtdataset)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.111111,\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Split: {SPLIT_NAME}\")\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
    "print(f\"Actual Split: {len(X_train)/len(X)*100:.1f}% / {len(X_val)/len(X)*100:.1f}% / {len(X_test)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf8460",
   "metadata": {},
   "source": [
    "## 3) Tokenization + Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256  # 256 ist oft ein guter Tradeoff bei News-Texten\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f81c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenize_texts(texts)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_ds = TextDataset(X_train, y_train)\n",
    "val_ds   = TextDataset(X_val, y_val)\n",
    "test_ds  = TextDataset(X_test, y_test)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f492b1e",
   "metadata": {},
   "source": [
    "## 4) Model + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5319f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels_sorted),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = PROJECT_ROOT / \"results\" / f\"distilbert_{SPLIT_NAME}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17489f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "train_output = trainer.train()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {elapsed/60:.1f} minutes\")\n",
    "train_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e925d",
   "metadata": {},
   "source": [
    "## 5) Evaluation auf Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11180f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = trainer.evaluate(val_ds)\n",
    "acc_val = val_result[\"eval_accuracy\"]\n",
    "print(f\"Validation Accuracy: {acc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece42ac5",
   "metadata": {},
   "source": [
    "## 6) Finale Evaluation auf Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cefb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(test_ds)\n",
    "test_logits = pred.predictions\n",
    "test_labels = pred.label_ids\n",
    "test_preds = np.argmax(test_logits, axis=-1)\n",
    "\n",
    "acc_test = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"FINALE TEST-SET EVALUATION (Split: {SPLIT_NAME})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {acc_test:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    target_names=[id2label[i] for i in range(len(labels_sorted))]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f4aae",
   "metadata": {},
   "source": [
    "## 7) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=list(range(len(labels_sorted))))\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=[id2label[i] for i in range(len(labels_sorted))],\n",
    "            yticklabels=[id2label[i] for i in range(len(labels_sorted))])\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(f\"Normalized Confusion Matrix – DistilBERT (TEST, {SPLIT_NAME})\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = OUT_DIR / f\"confusion_matrix_{MODEL_NAME.replace('/', '_')}_{SPLIT_NAME}.png\"\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "print(\"Saved:\", fig_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a651a",
   "metadata": {},
   "source": [
    "## 8) Model speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = OUT_DIR / \"best_model\"\n",
    "trainer.save_model(str(model_dir))\n",
    "tokenizer.save_pretrained(str(model_dir))\n",
    "\n",
    "print(\"Saved model to:\", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6d3d8",
   "metadata": {},
   "source": [
    "## 9) Ergebnisse speichern (für Vergleich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8663dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"model\": f\"DistilBERT ({MODEL_NAME})\",\n",
    "    \"split\": SPLIT_NAME,\n",
    "    \"train_size\": len(X_train),\n",
    "    \"val_size\": len(X_val),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"val_accuracy\": acc_val,\n",
    "    \"test_accuracy\": acc_test,\n",
    "    \"test_macro_f1\": f1_score(test_labels, test_preds, average=\"macro\"),\n",
    "    \"test_macro_precision\": precision_score(test_labels, test_preds, average=\"macro\"),\n",
    "    \"test_macro_recall\": recall_score(test_labels, test_preds, average=\"macro\"),\n",
    "    \"training_time_min\": round(elapsed / 60, 2),\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"epochs\": 3,\n",
    "    \"learning_rate\": 2e-5\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results_dict])\n",
    "results_path = PROJECT_ROOT / \"results\" / f\"distilbert_results_{SPLIT_NAME}.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"✅ Ergebnisse gespeichert: {results_path}\")\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
